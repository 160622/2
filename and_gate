import numpy as np

# Input features and target output for AND gate
input_features = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
target_output = np.array([0, 0, 0, 1])

# Initialize weights and bias
weights = np.array([0.1, 0.2])
bias = 0.3
lr = 0.1 # Learning rate

# Activation function and its derivative
def tanh(x):
    return np.tanh(x)

def tanh_derivative(x): # x here is the output of tanh
    return 1 - x**2

# Training loop
for epoch in range(10000):
    total_error = 0
    for inputs, label in zip(input_features, target_output):
        weighted_sum = np.dot(inputs, weights) + bias
        output = tanh(weighted_sum)

        error = label - output
        total_error += abs(error)

        delta = error * tanh_derivative(output)
        weights += lr * delta * inputs
        bias += lr * delta

    if total_error < 0.01:
        break

print("Final weights:", weights)
print("Final bias:", bias)

# Testing the trained perceptron
print("Testing the trained perceptron (using threshold at 0.5):")
for inputs, label in zip(input_features, target_output):
    output = tanh(np.dot(inputs, weights) + bias)
    predicted_label = 1 if output >= 0.5 else 0
    print(f"Input: {inputs}, Predicted Output: {predicted_label}, Target Output: {label}")
